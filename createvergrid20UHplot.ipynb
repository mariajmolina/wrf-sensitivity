{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "from wrf import to_np, getvar, get_basemap, latlon_coords\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "import math\n",
    "import xarray as xr\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "\n",
    "def getPathLength(lat1,lng1,lat2,lng2):\n",
    "    '''calculates the distance between two lat, long coordinate pairs'''\n",
    "    R = 6371000 # radius of earth in m\n",
    "    lat1rads = math.radians(lat1)\n",
    "    lat2rads = math.radians(lat2)\n",
    "    deltaLat = math.radians((lat2-lat1))\n",
    "    deltaLng = math.radians((lng2-lng1))\n",
    "    a = math.sin(deltaLat/2) * math.sin(deltaLat/2) + math.cos(lat1rads) * math.cos(lat2rads) * math.sin(deltaLng/2) * math.sin(deltaLng/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = R * c\n",
    "    return d\n",
    "\n",
    "def getDestinationLatLong(lat,lng,azimuth,distance):\n",
    "    '''returns the lat an long of destination point \n",
    "    given the start lat, long, aziuth, and distance'''\n",
    "    R = 6378.1 #Radius of the Earth in km\n",
    "    brng = math.radians(azimuth) #Bearing is degrees converted to radians.\n",
    "    d = distance/1000 #Distance m converted to km\n",
    "    lat1 = math.radians(lat) #Current dd lat point converted to radians\n",
    "    lon1 = math.radians(lng) #Current dd long point converted to radians\n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(d/R) + math.cos(lat1)* math.sin(d/R)* math.cos(brng))\n",
    "    lon2 = lon1 + math.atan2(math.sin(brng) * math.sin(d/R)* math.cos(lat1), math.cos(d/R)- math.sin(lat1)* math.sin(lat2))\n",
    "    #convert back to degrees\n",
    "    lat2 = math.degrees(lat2)\n",
    "    lon2 = math.degrees(lon2)\n",
    "    return[lat2, lon2]\n",
    "\n",
    "def calculateBearing(lat1,lng1,lat2,lng2):\n",
    "    '''calculates the azimuth in degrees from start point to end point'''\n",
    "    startLat = math.radians(lat1)\n",
    "    startLong = math.radians(lng1)\n",
    "    endLat = math.radians(lat2)\n",
    "    endLong = math.radians(lng2)\n",
    "    dLong = endLong - startLong\n",
    "    dPhi = math.log(math.tan(endLat/2.0+math.pi/4.0)/math.tan(startLat/2.0+math.pi/4.0))\n",
    "    if abs(dLong) > math.pi:\n",
    "         if dLong > 0.0:\n",
    "             dLong = -(2.0 * math.pi - dLong)\n",
    "         else:\n",
    "             dLong = (2.0 * math.pi + dLong)\n",
    "    bearing = (math.degrees(math.atan2(dLong, dPhi)) + 360.0) % 360.0;\n",
    "    return bearing\n",
    "\n",
    "def main(interval,lat1,lng1,lat2,lng2):\n",
    "    '''returns every coordinate pair inbetween two coordinate \n",
    "    pairs given the desired interval (in meters!).'''\n",
    "\n",
    "    d = getPathLength(lat1,lng1,lat2,lng2)\n",
    "    remainder, dist = math.modf((d / interval))\n",
    "    counter = float(interval)\n",
    "    coords = []\n",
    "    coords.append([lat1,lng1])\n",
    "    \n",
    "    azimuth = calculateBearing(lat1,lng1,lat2,lng2)\n",
    "    \n",
    "    for distance in range(0,int(dist)):\n",
    "        coord = getDestinationLatLong(lat1,lng1,azimuth,counter)\n",
    "        counter = counter + float(interval)\n",
    "        coords.append(coord)\n",
    "    coords.append([lat2,lng2])\n",
    "    return np.array(coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Opened...\n",
      "Tors finished.\n",
      "2017-01-21 09:00:00 complete...\n",
      "2017-01-21 10:00:00 complete...\n",
      "2017-01-21 11:00:00 complete...\n",
      "2017-01-21 12:00:00 complete...\n",
      "2017-01-21 13:00:00 complete...\n",
      "2017-01-21 14:00:00 complete...\n",
      "2017-01-21 15:00:00 complete...\n",
      "2017-01-21 16:00:00 complete...\n",
      "2017-01-21 17:00:00 complete...\n",
      "2017-01-21 18:00:00 complete...\n",
      "2017-01-21 19:00:00 complete...\n",
      "2017-01-21 20:00:00 complete...\n",
      "2017-01-21 21:00:00 complete...\n",
      "2017-01-21 22:00:00 complete...\n",
      "2017-01-21 23:00:00 complete...\n",
      "2017-01-22 00:00:00 complete...\n",
      "2017-01-22 01:00:00 complete...\n",
      "2017-01-22 02:00:00 complete...\n",
      "2017-01-22 03:00:00 complete...\n",
      "2017-01-22 04:00:00 complete...\n",
      "2017-01-22 05:00:00 complete...\n",
      "2017-01-22 06:00:00 complete...\n",
      "2017-01-22 07:00:00 complete...\n",
      "2017-01-22 08:00:00 complete...\n",
      "2017-01-22 09:00:00 complete...\n",
      "2017-01-22 10:00:00 complete...\n",
      "2017-01-22 11:00:00 complete...\n",
      "2017-01-22 12:00:00 complete...\n",
      "2017-01-22 13:00:00 complete...\n",
      "2017-01-22 14:00:00 complete...\n",
      "2017-01-22 15:00:00 complete...\n",
      "2017-01-22 16:00:00 complete...\n",
      "2017-01-22 17:00:00 complete...\n",
      "2017-01-22 18:00:00 complete...\n",
      "2017-01-22 19:00:00 complete...\n",
      "2017-01-22 20:00:00 complete...\n",
      "2017-01-22 21:00:00 complete...\n",
      "2017-01-22 22:00:00 complete...\n",
      "2017-01-22 23:00:00 complete...\n",
      "2017-01-23 00:00:00 complete...\n",
      "2017-01-23 01:00:00 complete...\n",
      "2017-01-23 02:00:00 complete...\n",
      "2017-01-23 03:00:00 complete...\n",
      "2017-01-23 04:00:00 complete...\n",
      "2017-01-23 05:00:00 complete...\n",
      "2017-01-23 06:00:00 complete...\n",
      "2017-01-23 07:00:00 complete...\n",
      "2017-01-23 08:00:00 complete...\n",
      "UH and DBZ finished.\n",
      "File Opened...\n",
      "Tors finished.\n",
      "2017-01-21 09:00:00 complete...\n",
      "2017-01-21 10:00:00 complete...\n",
      "2017-01-21 11:00:00 complete...\n",
      "2017-01-21 12:00:00 complete...\n",
      "2017-01-21 13:00:00 complete...\n",
      "2017-01-21 14:00:00 complete...\n",
      "2017-01-21 15:00:00 complete...\n",
      "2017-01-21 16:00:00 complete...\n",
      "2017-01-21 17:00:00 complete...\n",
      "2017-01-21 18:00:00 complete...\n",
      "2017-01-21 19:00:00 complete...\n",
      "2017-01-21 20:00:00 complete...\n",
      "2017-01-21 21:00:00 complete...\n",
      "2017-01-21 22:00:00 complete...\n",
      "2017-01-21 23:00:00 complete...\n",
      "2017-01-22 00:00:00 complete...\n",
      "2017-01-22 01:00:00 complete...\n",
      "2017-01-22 02:00:00 complete...\n",
      "2017-01-22 03:00:00 complete...\n",
      "2017-01-22 04:00:00 complete...\n",
      "2017-01-22 05:00:00 complete...\n",
      "2017-01-22 06:00:00 complete...\n",
      "2017-01-22 07:00:00 complete...\n",
      "2017-01-22 08:00:00 complete...\n",
      "2017-01-22 09:00:00 complete...\n",
      "2017-01-22 10:00:00 complete...\n",
      "2017-01-22 11:00:00 complete...\n",
      "2017-01-22 12:00:00 complete...\n",
      "2017-01-22 13:00:00 complete...\n",
      "2017-01-22 14:00:00 complete...\n",
      "2017-01-22 15:00:00 complete...\n",
      "2017-01-22 16:00:00 complete...\n",
      "2017-01-22 17:00:00 complete...\n",
      "2017-01-22 18:00:00 complete...\n",
      "2017-01-22 19:00:00 complete...\n",
      "2017-01-22 20:00:00 complete...\n",
      "2017-01-22 21:00:00 complete...\n",
      "2017-01-22 22:00:00 complete...\n",
      "2017-01-22 23:00:00 complete...\n"
     ]
    }
   ],
   "source": [
    "#computation choosing\n",
    "\n",
    "wrf_file_folder_choice = 'wrf4km_ens_1'\n",
    "#wrf_file_folder_choice = 'wrf4km_ens_2'\n",
    "#wrf_file_folder_choice = 'wrf4km_ens_3'\n",
    "#wrf_file_folder_choice = 'wrf4km_ens_4'\n",
    "#wrf_file_folder_choice = 'wrf4km_ens_5'\n",
    "#wrf_file_folder_choice = 'wrf4km_ens_6'\n",
    "\n",
    "choice_res = np.array([20])[0]\n",
    "\n",
    "UH_thresholds = np.array([80,80])\n",
    "#UH_thresholds = np.array([60,60,60,40,40,40])\n",
    "\n",
    "dbz_thresholds = np.array([40,30])\n",
    "\n",
    "\n",
    "Tors = pd.read_csv(r'/glade/work/molina/DATA/jan2017_synoptic/1950-2017_actual_tornadoes_fixed.csv',\n",
    "                   error_bad_lines=False,\n",
    "                   parse_dates=[['mo','dy','yr','time']])\n",
    "\n",
    "Tors = Tors.assign(UTC_time = Tors['mo_dy_yr_time'] + timedelta(hours=6))       \n",
    "Tors = Tors.assign(UTC_yr = Tors['UTC_time'].dt.year) \n",
    "Tors = Tors.assign(UTC_dy = Tors['UTC_time'].dt.day) \n",
    "Tors = Tors.assign(UTC_mo = Tors['UTC_time'].dt.month) \n",
    "Tors = Tors.assign(UTC_hr = Tors['UTC_time'].dt.hour) \n",
    "\n",
    "Tors = Tors[(Tors['UTC_mo']==1)]\n",
    "Tors = Tors[(Tors['UTC_yr']==2017)]\n",
    "\n",
    "Tors = Tors[(Tors['UTC_dy']==21)&(Tors['UTC_hr']>=9)|\n",
    "            (Tors['UTC_dy']==22)|\n",
    "            (Tors['UTC_dy']==23)&(Tors['UTC_hr']<=9)]\n",
    "\n",
    "Tors = Tors[Tors['slat']!=0]\n",
    "Tors = Tors[Tors['slon']!=0]\n",
    "Tors = Tors[(Tors['slat']>=20) & (Tors['slat']<=50)]\n",
    "Tors = Tors[(Tors['slon']>=-130) & (Tors['slon']<=-65)]\n",
    "\n",
    "T_elat = [Tors['slat'].values[u] if i==0 else i for u, i in enumerate(Tors['elat'].values)]\n",
    "T_elon = [Tors['slon'].values[u] if i==0 else i for u, i in enumerate(Tors['elon'].values)]\n",
    "\n",
    "Tors = Tors.assign(corr_elat = T_elat)\n",
    "Tors = Tors.assign(corr_elon = T_elon)\n",
    "\n",
    "Tors = Tors.set_index(['UTC_time']) \n",
    "\n",
    "\n",
    "for UH_threshold, dbz_threshold in zip(UH_thresholds, dbz_thresholds):\n",
    "\n",
    "    file2_wrf_d02 = '/glade/scratch/molina/WRF_HYSPLIT_proj/'+wrf_file_folder_choice+'/wrfout_d01_2017-01-15_09:00:00'\n",
    "    ncfile2_wrf_d02 = Dataset(file2_wrf_d02)\n",
    "    while True:\n",
    "        dbz2_wrf_d02 = np.array([0])\n",
    "        if len(dbz2_wrf_d02) == 1:\n",
    "            try: \n",
    "                dbz2_wrf_d02 = getvar(ncfile2_wrf_d02, \"dbz\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if len(dbz2_wrf_d02) != 1:\n",
    "            print('File Opened...')\n",
    "            break\n",
    "    lats4, lons4 = latlon_coords(dbz2_wrf_d02)\n",
    "    m = get_basemap(dbz2_wrf_d02)\n",
    "    xMax, yMax = m(m.urcrnrlon, m.urcrnrlat) \n",
    "    xMin, yMin = m(m.llcrnrlon, m.llcrnrlat) \n",
    "    x_range = (xMax-xMin) / 1000 \n",
    "    y_range = (yMax-yMin) / 1000 \n",
    "    \n",
    "    numXGrids = round(x_range / choice_res + .5,0) \n",
    "    numYGrids = round(y_range / choice_res + .5,0)\n",
    "    xi = np.linspace(xMin, xMax, int(numXGrids))\n",
    "    yi = np.linspace(yMin, yMax, int(numYGrids))\n",
    "    aggregate_grid = np.zeros((yi.shape[0]-1, xi.shape[0]-1)) \n",
    "\n",
    "    for tor_lat1, tor_lon1, tor_lat2, tor_lon2 in zip(Tors['slat'].values, Tors['slon'].values, Tors['corr_elat'].values, Tors['corr_elon'].values): \n",
    "\n",
    "        if tor_lat1 == tor_lat2:\n",
    "            x_values = tor_lon1\n",
    "            y_values = tor_lat1\n",
    "            x_proj, y_proj = m(x_values, y_values) \n",
    "            grid, _, _ = np.histogram2d([y_proj], [x_proj], bins=[yi, xi]) \n",
    "            aggregate_grid[:,:] += grid > 0 \n",
    "\n",
    "        if tor_lat1 != tor_lat2:\n",
    "\n",
    "            coords = main(10000, tor_lat1, tor_lon1, tor_lat2, tor_lon2)        \n",
    "            y_values = coords[:,0]\n",
    "            x_values = coords[:,1]\n",
    "            x_proj, y_proj = m(x_values, y_values)\n",
    "            grid, _, _ = np.histogram2d(y_proj, x_proj, bins=[yi, xi]) \n",
    "            aggregate_grid[:,:] += grid > 0\n",
    "        \n",
    "    print('Tors finished.')\n",
    "    \n",
    "    \n",
    "    #########################################################################################\n",
    "    #########################################################################################\n",
    "    #########################################################################################\n",
    "\n",
    "\n",
    "    #create the range of dates to loop thru wrf files\n",
    "    begdate = datetime.datetime.strptime('2017012109',\"%Y%m%d%H\") \n",
    "    enddate = datetime.datetime.strptime('2017012309',\"%Y%m%d%H\") \n",
    "    monthstr = datetime.datetime.strftime(begdate,\"%B\")\n",
    "    #format into strings\n",
    "    bmonth = '%02d' % begdate.month\n",
    "    bday = '%02d' % begdate.day\n",
    "    bbday = begdate.day\n",
    "    #loop thru hours\n",
    "    dates = []\n",
    "    while begdate < enddate:\n",
    "      dates.append(begdate)\n",
    "      begdate+=datetime.timedelta(hours=1) \n",
    "\n",
    "    uvv_coarse = np.zeros((len(dates),yi.shape[0]-1, xi.shape[0]-1))\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for count, dt in enumerate(dates):\n",
    "\n",
    "      YY = dt.year\n",
    "\n",
    "      MM = \"%02d\"%dt.month\n",
    "      DD = \"%02d\"%dt.day\n",
    "      HH = \"%02d\"%dt.hour\n",
    "\n",
    "      HH_current = dt.hour\n",
    "\n",
    "      if count != 0:\n",
    "          if HH_current != HH_previous:    \n",
    "              counter += 1\n",
    "\n",
    "      wrffile = '/glade/scratch/molina/WRF_HYSPLIT_proj/'+wrf_file_folder_choice+'/wrfout_d01_2017-'+MM+'-'+DD+'_'+HH+':00:00'\n",
    "\n",
    "      if os.path.isfile(wrffile)==True:\n",
    "\n",
    "        ncfile = Dataset(wrffile)\n",
    "\n",
    "        uvv = getvar(ncfile, \"updraft_helicity\", meta=False)\n",
    "        uvv_val = uvv\n",
    "\n",
    "        uvv2 = xr.open_dataset(wrffile, decode_cf=True).UP_HELI_MAX.values[0,:,:]\n",
    "        uvv2_val = uvv2\n",
    "\n",
    "        dbz = getvar(ncfile, \"dbz\", meta=False)\n",
    "        dbz_val = dbz[0,:,:]\n",
    "\n",
    "        for i, j in product(range(0,uvv2.shape[0]), range(0,uvv2.shape[1])):\n",
    "\n",
    "            max_UH = np.nanmax([uvv_val[i,j],uvv2_val[i,j]])\n",
    "\n",
    "            if max_UH >= UH_threshold and dbz_val[i,j] >= dbz_threshold:\n",
    "\n",
    "                latsnow, lonsnow = to_np(lats4)[i,j], to_np(lons4)[i,j]\n",
    "                x_proj,y_proj = m(lonsnow, latsnow)\n",
    "                grid, _, _ = np.histogram2d(np.array([y_proj]),np.array([x_proj]), bins=[yi,xi])\n",
    "\n",
    "                uvv_coarse[counter,:,:] += grid > 0\n",
    "\n",
    "      else:\n",
    "\n",
    "        print(\"file doesn't exist\", os.path.basename(ncfile))\n",
    "\n",
    "      HH_previous = dt.hour\n",
    "\n",
    "      print(str(dt)+' complete...')\n",
    "\n",
    "    print('UH and DBZ finished.')\n",
    "\n",
    "    x1, y1 = xi, yi\n",
    "\n",
    "    x2 = np.zeros(x1.shape)\n",
    "    y2 = np.zeros(y1.shape)\n",
    "    x3 = np.zeros(x1.shape)\n",
    "    y3 = np.zeros(y1.shape)\n",
    "\n",
    "    for i, j in product(range(len(x1)),range(len(y1))):\n",
    "\n",
    "        x2[i], y2[j] = m(x1[i], y1[j], inverse=True)\n",
    "\n",
    "        x2[i] = x2[i]-0.4\n",
    "        y2[j] = y2[j]-0.4\n",
    "\n",
    "        x3[i], y3[j] = m(x2[i], y2[j])\n",
    "\n",
    "    file_tors = np.where(aggregate_grid == 0., aggregate_grid, 1)\n",
    "\n",
    "    file_uhdbz = np.where(np.nansum(uvv_coarse, axis=0) == 0., np.nansum(uvv_coarse, axis=0), 1)        \n",
    "\n",
    "    x4, y4 = x3[1:], y3[1:]\n",
    "\n",
    "    uvv_coarse1 = np.nansum(uvv_coarse, axis=0)\n",
    "\n",
    "\n",
    "    the_mask = xr.open_dataset('/glade/work/molina/DATA/usstates_shapefiles/conus_mask_res_20.nc')\n",
    "\n",
    "\n",
    "    uhdbz_forsns_notone = np.where(the_mask['conus'].values, uvv_coarse1, 0)\n",
    "    uhdbz_forsns_one = np.where(the_mask['conus'].values, file_uhdbz, 0)\n",
    "    tors_forsns_notone = np.where(the_mask['conus'].values, aggregate_grid, 0)\n",
    "    tors_forsns_one = np.where(the_mask['conus'].values, file_tors, 0)    \n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    ###############################################################################\n",
    "    ###############################################################################\n",
    "\n",
    "\n",
    "    final_filed = xr.Dataset({'UHdbz_nomask':(['y','x'], uvv_coarse1),\n",
    "                              'UHdbz_mask':(['y','x'], uhdbz_forsns_notone),\n",
    "                              'UHdbz_one_nomask':(['y','x'], file_uhdbz),\n",
    "                              'UHdbz_one_mask':(['y','x'], uhdbz_forsns_one),\n",
    "\n",
    "                              'tors_nomask':(['y','x'], aggregate_grid),\n",
    "                              'tors_mask':(['y','x'], tors_forsns_notone),\n",
    "                              'tors_one_nomask':(['y','x'], file_tors),\n",
    "                              'tors_one_mask':(['y','x'], tors_forsns_one),\n",
    "\n",
    "                              },\n",
    "\n",
    "                              coords={'lon':(['x'],x4),\n",
    "                                      'lat':(['y'],y4)},\n",
    "\n",
    "                              attrs={'File Author':'Maria J. Molina'})\n",
    "\n",
    "\n",
    "    final_filed.to_netcdf('/glade/work/molina/DATA/jan2017_synoptic/'+wrf_file_folder_choice+'/ver_grid_20_UH_'+str(UH_threshold)+'_Z_'+str(dbz_threshold)+'.nc')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-python-tutorial]",
   "language": "python",
   "name": "conda-env-miniconda3-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
